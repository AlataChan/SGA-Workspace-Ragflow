å¥½çš„ï¼çœ‹æ¥Dockeræ„å»ºçš„æ–¹å¼æ¯”è¾ƒå¤æ‚ï¼Œè®©æˆ‘ç›´æ¥ä¸ºæ‚¨æä¾›RAGFlowçŸ¥è¯†å›¾è°±APIçš„ä½¿ç”¨æŒ‡å—ã€‚

## ğŸ“š RAGFlowçŸ¥è¯†å›¾è°±APIä½¿ç”¨æŒ‡å—

### ğŸ”‘ APIè®¤è¯

é¦–å…ˆéœ€è¦è·å–APIå¯†é’¥ï¼š

1. ç™»å½•RAGFlowç•Œé¢ï¼šhttp://localhost:9380
2. è¿›å…¥è®¾ç½®é¡µé¢è·å–API Key
3. åœ¨è¯·æ±‚å¤´ä¸­æ·»åŠ è®¤è¯ï¼š`Authorization: Bearer <YOUR_API_KEY>`

### ğŸŒ ä¸»è¦APIæ¥å£

#### 1. **è·å–çŸ¥è¯†å›¾è°±æ•°æ®**
```bash
# è·å–å®Œæ•´çš„çŸ¥è¯†å›¾è°±
curl -X GET "http://localhost:9380/api/v1/graphrag/kb/{kb_id}/graph" \
  -H "Authorization: Bearer <YOUR_API_KEY>" \
  -H "Content-Type: application/json"
```

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "retcode": 0,
  "retmsg": "success",
  "data": {
    "graph": {
      "nodes": [
        {
          "id": "node_123",
          "entity_type": "PERSON",
          "description": "äººå·¥æ™ºèƒ½ä¸“å®¶",
          "pagerank": 0.85,
          "communities": ["tech", "ai"]
        }
      ],
      "edges": [
        {
          "source": "node_123",
          "target": "node_456",
          "relation": "works_with",
          "weight": 0.7
        }
      ]
    },
    "kb_info": {
      "id": "kb_123",
      "name": "çŸ¥è¯†åº“åç§°"
    }
  }
}
```

#### 2. **æœç´¢èŠ‚ç‚¹**
```bash
# æœç´¢çŸ¥è¯†å›¾è°±ä¸­çš„èŠ‚ç‚¹
curl -X POST "http://localhost:9380/api/v1/graphrag/kb/{kb_id}/search" \
  -H "Authorization: Bearer <YOUR_API_KEY>" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "äººå·¥æ™ºèƒ½",
    "entity_types": ["PERSON", "ORGANIZATION", "CONCEPT"],
    "page": 1,
    "page_size": 20
  }'
```

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "retcode": 0,
  "retmsg": "success",
  "data": {
    "nodes": [
      {
        "id": "node_123",
        "entity_type": "CONCEPT",
        "description": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "pagerank": 0.92,
        "communities": ["technology", "science"]
      }
    ],
    "total_count": 45,
    "page": 1,
    "page_size": 20,
    "has_more": true
  }
}
```

#### 3. **è·å–èŠ‚ç‚¹å…³è”æ–‡ä»¶**
```bash
# è·å–èŠ‚ç‚¹çš„å…³è”æ–‡ä»¶åˆ—è¡¨
curl -X GET "http://localhost:9380/api/v1/graphrag/kb/{kb_id}/nodes/{node_id}/files" \
  -H "Authorization: Bearer <YOUR_API_KEY>"
```

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "retcode": 0,
  "retmsg": "success",
  "data": {
    "files": [
      {
        "file_id": "file_123",
        "file_name": "AIç ”ç©¶æŠ¥å‘Š.pdf",
        "file_type": "pdf",
        "chunk_count": 15,
        "relevance_score": 0.89
      }
    ],
    "total_files": 3
  }
}
```

#### 4. **ä¸‹è½½èŠ‚ç‚¹å†…å®¹**
```bash
# ä¸‹è½½èŠ‚ç‚¹å†…å®¹ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
curl -X GET "http://localhost:9380/api/v1/graphrag/kb/{kb_id}/nodes/{node_id}/download?format=json" \
  -H "Authorization: Bearer <YOUR_API_KEY>" \
  -o "node_content.json"

# æ”¯æŒçš„æ ¼å¼ï¼štxt, json, csv, xlsx
```

#### 5. **è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯**
```bash
# è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡æ•°æ®
curl -X GET "http://localhost:9380/api/v1/graphrag/kb/{kb_id}/statistics" \
  -H "Authorization: Bearer <YOUR_API_KEY>"
```

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "retcode": 0,
  "retmsg": "success",
  "data": {
    "statistics": {
      "total_nodes": 1250,
      "total_edges": 3400,
      "entity_types": {
        "PERSON": 320,
        "ORGANIZATION": 180,
        "CONCEPT": 750
      },
      "avg_degree": 2.72,
      "density": 0.0043
    }
  }
}
```

### ğŸ Python SDKä½¿ç”¨ç¤ºä¾‹

#### å®‰è£…SDK
```bash
pip install ragflow-sdk
```

#### åŸºæœ¬ä½¿ç”¨
```python
import asyncio
from ragflow_sdk import RAGFlow

async def main():
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    rag = RAGFlow(
        api_key="your-api-key",
        base_url="http://localhost:9380"
    )
    
    # è·å–çŸ¥è¯†åº“åˆ—è¡¨
    datasets = rag.list_datasets()
    kb_id = datasets[0].id
    
    # 1. è·å–å®Œæ•´çŸ¥è¯†å›¾è°±
    print("=== è·å–çŸ¥è¯†å›¾è°± ===")
    response = await rag.get(f"/api/v1/graphrag/kb/{kb_id}/graph")
    graph_data = response.json()
    print(f"èŠ‚ç‚¹æ•°é‡: {len(graph_data['data']['graph']['nodes'])}")
    print(f"è¾¹æ•°é‡: {len(graph_data['data']['graph']['edges'])}")
    
    # 2. æœç´¢èŠ‚ç‚¹
    print("\n=== æœç´¢èŠ‚ç‚¹ ===")
    search_payload = {
        "query": "äººå·¥æ™ºèƒ½",
        "entity_types": ["CONCEPT", "PERSON"],
        "page": 1,
        "page_size": 10
    }
    response = await rag.post(f"/api/v1/graphrag/kb/{kb_id}/search", json=search_payload)
    search_results = response.json()
    
    for node in search_results['data']['nodes']:
        print(f"èŠ‚ç‚¹ID: {node['id']}")
        print(f"ç±»å‹: {node['entity_type']}")
        print(f"æè¿°: {node['description'][:100]}...")
        print(f"é‡è¦æ€§: {node.get('pagerank', 'N/A')}")
        print("---")
    
    # 3. è·å–èŠ‚ç‚¹å…³è”æ–‡ä»¶
    if search_results['data']['nodes']:
        node_id = search_results['data']['nodes'][0]['id']
        print(f"\n=== èŠ‚ç‚¹ {node_id} çš„å…³è”æ–‡ä»¶ ===")
        response = await rag.get(f"/api/v1/graphrag/kb/{kb_id}/nodes/{node_id}/files")
        files_data = response.json()
        
        for file_info in files_data['data']['files']:
            print(f"æ–‡ä»¶: {file_info['file_name']}")
            print(f"ç±»å‹: {file_info['file_type']}")
            print(f"ç›¸å…³æ€§: {file_info['relevance_score']}")
            print("---")
    
    # 4. è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\n=== å›¾è°±ç»Ÿè®¡ä¿¡æ¯ ===")
    response = await rag.get(f"/api/v1/graphrag/kb/{kb_id}/statistics")
    stats = response.json()
    
    statistics = stats['data']['statistics']
    print(f"æ€»èŠ‚ç‚¹æ•°: {statistics['total_nodes']}")
    print(f"æ€»è¾¹æ•°: {statistics['total_edges']}")
    print(f"å¹³å‡åº¦: {statistics['avg_degree']}")
    print(f"å›¾å¯†åº¦: {statistics['density']}")
    
    print("\nå®ä½“ç±»å‹åˆ†å¸ƒ:")
    for entity_type, count in statistics['entity_types'].items():
        print(f"  {entity_type}: {count}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(main())
```

### ğŸ”§ é«˜çº§ç”¨æ³•ç¤ºä¾‹

#### 1. **æ‰¹é‡æœç´¢å’Œè¿‡æ»¤**
```python
async def advanced_search(rag, kb_id):
    """é«˜çº§æœç´¢ç¤ºä¾‹"""
    
    # æœç´¢ç‰¹å®šç±»å‹çš„å®ä½“
    entity_types = ["PERSON", "ORGANIZATION", "CONCEPT"]
    
    for entity_type in entity_types:
        search_payload = {
            "query": "",  # ç©ºæŸ¥è¯¢è·å–æ‰€æœ‰è¯¥ç±»å‹å®ä½“
            "entity_types": [entity_type],
            "page": 1,
            "page_size": 50
        }
        
        response = await rag.post(f"/api/v1/graphrag/kb/{kb_id}/search", json=search_payload)
        results = response.json()
        
        print(f"\n{entity_type} ç±»å‹å®ä½“ (å…±{results['data']['total_count']}ä¸ª):")
        
        # æŒ‰é‡è¦æ€§æ’åº
        nodes = sorted(
            results['data']['nodes'], 
            key=lambda x: x.get('pagerank', 0), 
            reverse=True
        )
        
        for node in nodes[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæœ€é‡è¦çš„
            print(f"  {node['id']}: {node['description'][:50]}... (é‡è¦æ€§: {node.get('pagerank', 'N/A')})")
```

#### 2. **å›¾è°±æ•°æ®åˆ†æ**
```python
async def analyze_graph(rag, kb_id):
    """å›¾è°±æ•°æ®åˆ†æ"""
    
    # è·å–å®Œæ•´å›¾è°±
    response = await rag.get(f"/api/v1/graphrag/kb/{kb_id}/graph")
    graph_data = response.json()
    
    nodes = graph_data['data']['graph']['nodes']
    edges = graph_data['data']['graph']['edges']
    
    # åˆ†æèŠ‚ç‚¹åº¦åˆ†å¸ƒ
    degree_count = {}
    for edge in edges:
        source = edge['source']
        target = edge['target']
        degree_count[source] = degree_count.get(source, 0) + 1
        degree_count[target] = degree_count.get(target, 0) + 1
    
    # æ‰¾å‡ºè¿æ¥åº¦æœ€é«˜çš„èŠ‚ç‚¹
    top_nodes = sorted(degree_count.items(), key=lambda x: x[1], reverse=True)[:10]
    
    print("è¿æ¥åº¦æœ€é«˜çš„10ä¸ªèŠ‚ç‚¹:")
    for node_id, degree in top_nodes:
        # æ‰¾åˆ°èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
        node_info = next((n for n in nodes if n['id'] == node_id), None)
        if node_info:
            print(f"  {node_id}: {node_info['description'][:50]}... (åº¦: {degree})")
```

#### 3. **å†…å®¹ä¸‹è½½å’Œå¯¼å‡º**
```python
async def export_node_content(rag, kb_id, node_id, format="json"):
    """å¯¼å‡ºèŠ‚ç‚¹å†…å®¹"""
    
    # ä¸‹è½½èŠ‚ç‚¹å†…å®¹
    response = await rag.get(f"/api/v1/graphrag/kb/{kb_id}/nodes/{node_id}/download?format={format}")
    
    if format == "json":
        content = response.json()
        # ä¿å­˜åˆ°æ–‡ä»¶
        import json
        with open(f"node_{node_id}.json", "w", encoding="utf-8") as f:
            json.dump(content, f, ensure_ascii=False, indent=2)
    else:
        # å…¶ä»–æ ¼å¼ç›´æ¥ä¿å­˜äºŒè¿›åˆ¶å†…å®¹
        with open(f"node_{node_id}.{format}", "wb") as f:
            f.write(response.content)
    
    print(f"èŠ‚ç‚¹ {node_id} å†…å®¹å·²å¯¼å‡ºä¸º {format} æ ¼å¼")
```

### ğŸš€ å®é™…åº”ç”¨åœºæ™¯

#### 1. **çŸ¥è¯†å‘ç°**
```python
async def knowledge_discovery(rag, kb_id, topic):
    """åŸºäºä¸»é¢˜çš„çŸ¥è¯†å‘ç°"""
    
    # æœç´¢ç›¸å…³èŠ‚ç‚¹
    search_payload = {
        "query": topic,
        "entity_types": ["CONCEPT", "PERSON", "ORGANIZATION"],
        "page": 1,
        "page_size": 20
    }
    
    response = await rag.post(f"/api/v1/graphrag/kb/{kb_id}/search", json=search_payload)
    results = response.json()
    
    print(f"å…³äº '{topic}' çš„çŸ¥è¯†å‘ç°:")
    
    for node in results['data']['nodes']:
        print(f"\nèŠ‚ç‚¹: {node['id']}")
        print(f"ç±»å‹: {node['entity_type']}")
        print(f"æè¿°: {node['description']}")
        
        # è·å–å…³è”æ–‡ä»¶
        files_response = await rag.get(f"/api/v1/graphrag/kb/{kb_id}/nodes/{node['id']}/files")
        files_data = files_response.json()
        
        if files_data['data']['files']:
            print("ç›¸å…³æ–‡æ¡£:")
            for file_info in files_data['data']['files'][:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                print(f"  - {file_info['file_name']} (ç›¸å…³æ€§: {file_info['relevance_score']})")
```

#### 2. **å…³ç³»åˆ†æ**
```python
async def relationship_analysis(rag, kb_id):
    """å…³ç³»åˆ†æ"""
    
    # è·å–å›¾è°±æ•°æ®
    response = await rag.get(f"/api/v1/graphrag/kb/{kb_id}/graph")
    graph_data = response.json()
    
    edges = graph_data['data']['graph']['edges']
    
    # åˆ†æå…³ç³»ç±»å‹
    relation_types = {}
    for edge in edges:
        relation = edge.get('relation', 'unknown')
        relation_types[relation] = relation_types.get(relation, 0) + 1
    
    print("å…³ç³»ç±»å‹åˆ†å¸ƒ:")
    for relation, count in sorted(relation_types.items(), key=lambda x: x[1], reverse=True):
        print(f"  {relation}: {count}")
```

è¿™æ ·æ‚¨å°±å¯ä»¥é€šè¿‡APIå®Œå…¨æ§åˆ¶å’Œä½¿ç”¨RAGFlowçš„çŸ¥è¯†å›¾è°±åŠŸèƒ½äº†ï¼æ‰€æœ‰çš„å›¾è°±æ•°æ®ã€æœç´¢ã€æ–‡ä»¶å…³è”ç­‰åŠŸèƒ½éƒ½å¯ä»¥é€šè¿‡APIè®¿é—®ã€‚
