# RAGFlow çŸ¥è¯†å›¾è°± API ä½¿ç”¨æŒ‡å—

## ğŸ“š æ¦‚è¿°

RAGFlow æä¾›äº†å®Œæ•´çš„çŸ¥è¯†å›¾è°± API æ¥å£ï¼Œæ”¯æŒå›¾è°±æ•°æ®è·å–ã€èŠ‚ç‚¹æœç´¢ã€æ–‡ä»¶å…³è”å’Œå†…å®¹ä¸‹è½½ç­‰åŠŸèƒ½ã€‚

## ğŸ”‘ API è®¤è¯

### è·å– API å¯†é’¥
1. ç™»å½• RAGFlow ç•Œé¢ï¼šhttp://localhost:9380
2. è¿›å…¥è®¾ç½®é¡µé¢è·å– API Key
3. åœ¨è¯·æ±‚å¤´ä¸­æ·»åŠ è®¤è¯ï¼š`Authorization: Bearer <YOUR_API_KEY>`

### ç¤ºä¾‹ API å¯†é’¥
```
ragflow-BlMGQyNzM0OTBhNzExZjA4MzU4ZGU3NW
```

## ğŸ“‹ æ•°æ®é›†ç®¡ç†

### 1. åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†
```bash
curl -X GET "http://localhost:9380/api/v1/datasets" \
  -H "Authorization: Bearer ragflow-BlMGQyNzM0OTBhNzExZjA4MzU4ZGU3NW"
```

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "code": 0,
  "data": [
    {
      "id": "dc949110906a11f08b78aa7cd3e67281",
      "name": "å›½è´¸åˆ¶åº¦çŸ¥è¯†åº“",
      "document_count": 31,
      "chunk_count": 566,
      "parser_config": {
        "graphrag": {
          "use_graphrag": true,
          "method": "light",
          "entity_types": ["organization", "person", "geo", "event", "category"]
        }
      }
    }
  ]
}
```

## ğŸ•¸ï¸ çŸ¥è¯†å›¾è°± API

### 2. è·å–å®Œæ•´çŸ¥è¯†å›¾è°±
```bash
curl -X GET "http://localhost:9380/api/v1/datasets/{kb_id}/knowledge_graph" \
  -H "Authorization: Bearer ragflow-BlMGQyNzM0OTBhNzExZjA4MzU4ZGU3NW"
```

**å®é™…ç¤ºä¾‹ï¼š**
```bash
curl -X GET "http://localhost:9380/api/v1/datasets/dc949110906a11f08b78aa7cd3e67281/knowledge_graph" \
  -H "Authorization: Bearer ragflow-BlMGQyNzM0OTBhNzExZjA4MzU4ZGU3NW"
```

**å“åº”ç»“æ„ï¼š**
```json
{
  "code": 0,
  "data": {
    "graph": {
      "directed": false,
      "nodes": [
        {
          "entity_name": "è´¢åŠ¡éƒ¨",
          "entity_type": "ORGANIZATION",
          "id": "è´¢åŠ¡éƒ¨",
          "pagerank": 0.025,
          "description": "è´Ÿè´£è´¢åŠ¡å®¡æ ¸å’Œç®¡ç†çš„éƒ¨é—¨"
        }
      ],
      "edges": [
        {
          "source": "è´¢åŠ¡éƒ¨",
          "target": "é»„å‘å",
          "description": "é»„å‘åæ˜¯è´¢åŠ¡éƒ¨çš„å®¡æ ¸å‘˜",
          "weight": 39.0
        }
      ]
    }
  }
}
```

## ğŸ” å®ä½“ç±»å‹åˆ†æ

### ä¸»è¦å®ä½“ç±»å‹

#### ğŸ‘¥ äººå‘˜ (PERSON)
- **é»„å‘å**: è´¢åŠ¡éƒ¨å®¡æ ¸å‘˜ï¼Œæä¾›åˆæ­¥å®¡æ‰¹
- **å­™æ˜¥è‡£**: æ–‡æ¡£èµ·è‰äººï¼Œè´Ÿè´£å®‰å…¨ç®¡ç†è§„å®šæäº¤
- **çºªå‡Œéº’**: æ€»åŠæœ€ç»ˆå®¡æ‰¹äºº
- **å‘¨æ­¦æ¨**: è´¢åŠ¡éƒ¨æˆå‘˜ï¼Œå‚ä¸æ–‡æ¡£å®¡æ‰¹æµç¨‹

#### ğŸ¢ ç»„ç»‡ (ORGANIZATION)
- **å¦é—¨å›½è´¸è‚¡ä»½æœ‰é™å…¬å¸**: ä¸»ä½“å…¬å¸
- **è´¢åŠ¡éƒ¨**: è´Ÿè´£è´¢åŠ¡å®¡æ ¸å’Œç®¡ç†
- **æ³•å¾‹äº‹åŠ¡éƒ¨**: è´Ÿè´£æ³•å¾‹ç›¸å…³è´¹ç”¨å®¡æ‰¹
- **æ€»è£åŠå…¬å®¤**: æœ€ç»ˆå®¡æ‰¹æœºæ„

#### ğŸ“‹ ç±»åˆ« (CATEGORY)
- **å‡ºå·®å€Ÿæ¬¾**: éœ€è¦è´¢åŠ¡ç»ç†å®¡æ‰¹çš„è´¹ç”¨ç±»å‹
- **è¯‰è®¼è´¹**: éœ€è¦æ³•å¾‹äº‹åŠ¡éƒ¨é¢å¤–å®¡æ‰¹
- **å¤‡ç”¨é‡‘**: éœ€è¦åŒºåŸŸæ€»ç»ç†å®¡æ‰¹
- **ä¸šåŠ¡æ´»åŠ¨è´¹**: éœ€è¦è”åˆå®¡æ‰¹çš„è´¹ç”¨

#### ğŸŒ åœ°ç†ä½ç½® (GEO)
- **å¦é—¨**: å…¬å¸æ€»éƒ¨æ‰€åœ¨åœ°

## ğŸ”— å…³ç³»åˆ†æ

### å®¡æ‰¹æµç¨‹å…³ç³»
1. **èµ·è‰é˜¶æ®µ**: å­™æ˜¥è‡£ â†’ æ–‡æ¡£èµ·è‰
2. **åˆå®¡é˜¶æ®µ**: è´¢åŠ¡éƒ¨(é»„å‘å) â†’ åˆæ­¥å®¡æ‰¹
3. **ç»ˆå®¡é˜¶æ®µ**: æ€»åŠ(çºªå‡Œéº’) â†’ æœ€ç»ˆå®¡æ‰¹

### éƒ¨é—¨åä½œå…³ç³»
- è´¢åŠ¡éƒ¨ â†” åŒºåŸŸå…¬å¸æ€»ç»ç†ï¼šè”åˆå®¡æ‰¹æŸäº›è´¹ç”¨
- æ³•å¾‹äº‹åŠ¡éƒ¨ â†” è¯‰è®¼è´¹ï¼šä¸“é¡¹å®¡æ‰¹æƒé™
- æ€»è£åŠå…¬å®¤ â†” å„éƒ¨é—¨ï¼šæœ€ç»ˆå®¡æ‰¹æƒå¨

## ğŸš€ å®é™…åº”ç”¨ç¤ºä¾‹

### 1. æœç´¢è´¢åŠ¡ç›¸å…³å®ä½“
```bash
curl -X POST "http://localhost:9380/api/v1/datasets/dc949110906a11f08b78aa7cd3e67281/search" \
  -H "Authorization: Bearer ragflow-BlMGQyNzM0OTBhNzExZjA4MzU4ZGU3NW" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "è´¢åŠ¡",
    "entity_types": ["PERSON", "ORGANIZATION"],
    "page": 1,
    "page_size": 10
  }'
```

### 2. è·å–èŠ‚ç‚¹å…³è”æ–‡ä»¶
```bash
curl -X GET "http://localhost:9380/api/v1/datasets/dc949110906a11f08b78aa7cd3e67281/nodes/è´¢åŠ¡éƒ¨/files" \
  -H "Authorization: Bearer ragflow-BlMGQyNzM0OTBhNzExZjA4MzU4ZGU3NW"
```

### 3. ä¸‹è½½èŠ‚ç‚¹å†…å®¹
```bash
curl -X GET "http://localhost:9380/api/v1/datasets/dc949110906a11f08b78aa7cd3e67281/nodes/è´¢åŠ¡éƒ¨/download?format=json" \
  -H "Authorization: Bearer ragflow-BlMGQyNzM0OTBhNzExZjA4MzU4ZGU7NW" \
  -o "è´¢åŠ¡éƒ¨_content.json"
```

## ğŸ’¡ ä¸šåŠ¡åº”ç”¨åœºæ™¯

### 1. åˆè§„æ£€æŸ¥ç³»ç»Ÿ
- æŸ¥è¯¢ç‰¹å®šå®¡æ‰¹æµç¨‹å’Œè´£ä»»äºº
- éªŒè¯è´¹ç”¨å®¡æ‰¹æ˜¯å¦ç¬¦åˆè§„å®š
- è¿½è¸ªæ–‡æ¡£å®¡æ‰¹è·¯å¾„

### 2. ç»„ç»‡æ¶æ„åˆ†æ
- åˆ†æéƒ¨é—¨é—´åä½œå…³ç³»
- è¯†åˆ«å…³é”®å†³ç­–èŠ‚ç‚¹
- ä¼˜åŒ–å®¡æ‰¹æµç¨‹

### 3. é£é™©ç®¡ç†
- è¯†åˆ«å•ç‚¹æ•…éšœé£é™©
- åˆ†ææƒé™é›†ä¸­åº¦
- ç›‘æ§å¼‚å¸¸å®¡æ‰¹æ¨¡å¼

### 4. æ™ºèƒ½é—®ç­”
- "è°è´Ÿè´£å‡ºå·®å€Ÿæ¬¾å®¡æ‰¹ï¼Ÿ" â†’ è´¢åŠ¡ç»ç†
- "è¯‰è®¼è´¹éœ€è¦å“ªäº›éƒ¨é—¨å®¡æ‰¹ï¼Ÿ" â†’ æ³•å¾‹äº‹åŠ¡éƒ¨ + è´¢åŠ¡éƒ¨
- "å¦é—¨å›½è´¸çš„ç»„ç»‡æ¶æ„æ˜¯ä»€ä¹ˆï¼Ÿ" â†’ è¿”å›ç›¸å…³ç»„ç»‡å…³ç³»å›¾

## ğŸ› ï¸ Python SDK ç¤ºä¾‹

```python
import requests
import json

class RAGFlowKnowledgeGraph:
    def __init__(self, base_url, api_key):
        self.base_url = base_url
        self.headers = {"Authorization": f"Bearer {api_key}"}
    
    def get_datasets(self):
        """è·å–æ‰€æœ‰æ•°æ®é›†"""
        response = requests.get(f"{self.base_url}/api/v1/datasets", headers=self.headers)
        return response.json()
    
    def get_knowledge_graph(self, kb_id):
        """è·å–çŸ¥è¯†å›¾è°±"""
        response = requests.get(
            f"{self.base_url}/api/v1/datasets/{kb_id}/knowledge_graph", 
            headers=self.headers
        )
        return response.json()
    
    def search_entities(self, kb_id, query, entity_types=None):
        """æœç´¢å®ä½“"""
        data = {
            "query": query,
            "entity_types": entity_types or [],
            "page": 1,
            "page_size": 20
        }
        response = requests.post(
            f"{self.base_url}/api/v1/datasets/{kb_id}/search",
            headers=self.headers,
            json=data
        )
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
kg = RAGFlowKnowledgeGraph("http://localhost:9380", "ragflow-BlMGQyNzM0OTBhNzExZjA4MzU4ZGU3NW")

# è·å–æ•°æ®é›†
datasets = kg.get_datasets()
kb_id = datasets['data'][0]['id']

# è·å–çŸ¥è¯†å›¾è°±
graph = kg.get_knowledge_graph(kb_id)

# æœç´¢è´¢åŠ¡ç›¸å…³å®ä½“
results = kg.search_entities(kb_id, "è´¢åŠ¡", ["PERSON", "ORGANIZATION"])
```

## ğŸ“Š å›¾è°±æ•°æ®ç»Ÿè®¡

**å½“å‰çŸ¥è¯†åº“ç»Ÿè®¡**:
- èŠ‚ç‚¹æ•°é‡: 200+ ä¸ªå®ä½“
- è¾¹æ•°é‡: 100+ æ¡å…³ç³»
- æ–‡æ¡£æ•°é‡: 31 ä¸ª
- åˆ†å—æ•°é‡: 566 ä¸ª

**å®ä½“åˆ†å¸ƒ**:
- äººå‘˜ (PERSON): ~30%
- ç»„ç»‡ (ORGANIZATION): ~25%
- ç±»åˆ« (CATEGORY): ~35%
- åœ°ç†ä½ç½® (GEO): ~5%
- äº‹ä»¶ (EVENT): ~5%

## ğŸ”§ é…ç½®è¯´æ˜

### ç½‘ç»œé…ç½®
å¦‚æœä½¿ç”¨ frp å†…ç½‘ç©¿é€ï¼Œè¯·ç¡®ä¿é…ç½®æ­£ç¡®ï¼š
```toml
# frpc.toml
localIP = "host.docker.internal"
localPort = 9380
```

### API ç«¯ç‚¹
- åŸºç¡€URL: `http://localhost:9380`
- æ•°æ®é›†API: `/api/v1/datasets`
- çŸ¥è¯†å›¾è°±API: `/api/v1/datasets/{kb_id}/knowledge_graph`

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **API å¯†é’¥å®‰å…¨**: è¯·å¦¥å–„ä¿ç®¡ API å¯†é’¥ï¼Œé¿å…æ³„éœ²
2. **è¯·æ±‚é¢‘ç‡**: å»ºè®®æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…è¿‡è½½
3. **æ•°æ®æ›´æ–°**: çŸ¥è¯†å›¾è°±æ•°æ®ä¼šéšæ–‡æ¡£æ›´æ–°è€Œå˜åŒ–
4. **æƒé™æ§åˆ¶**: ç¡®ä¿åªæœ‰æˆæƒç”¨æˆ·å¯ä»¥è®¿é—®æ•æ„Ÿæ•°æ®

## ğŸ”„ é«˜çº§ç”¨æ³•

### æ‰¹é‡æ•°æ®åˆ†æ
```python
def analyze_approval_workflow(kg, kb_id):
    """åˆ†æå®¡æ‰¹å·¥ä½œæµç¨‹"""

    # è·å–å®Œæ•´å›¾è°±
    graph = kg.get_knowledge_graph(kb_id)
    nodes = graph['data']['graph']['nodes']
    edges = graph['data']['graph']['edges']

    # åˆ†æå®¡æ‰¹é“¾è·¯
    approval_chain = {}
    for edge in edges:
        if "å®¡æ‰¹" in edge.get('description', ''):
            source = edge['source']
            target = edge['target']
            approval_chain[source] = approval_chain.get(source, [])
            approval_chain[source].append(target)

    return approval_chain

def find_key_personnel(kg, kb_id):
    """è¯†åˆ«å…³é”®äººå‘˜"""

    # æœç´¢æ‰€æœ‰äººå‘˜
    results = kg.search_entities(kb_id, "", ["PERSON"])

    # æŒ‰é‡è¦æ€§æ’åº
    personnel = sorted(
        results['data']['nodes'],
        key=lambda x: x.get('pagerank', 0),
        reverse=True
    )

    return personnel[:10]  # è¿”å›å‰10ä¸ªé‡è¦äººå‘˜
```

### å®æ—¶ç›‘æ§ç¤ºä¾‹
```python
import time
import schedule

def monitor_graph_changes(kg, kb_id):
    """ç›‘æ§å›¾è°±å˜åŒ–"""

    def check_updates():
        try:
            graph = kg.get_knowledge_graph(kb_id)
            node_count = len(graph['data']['graph']['nodes'])
            edge_count = len(graph['data']['graph']['edges'])

            print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] "
                  f"èŠ‚ç‚¹: {node_count}, è¾¹: {edge_count}")

        except Exception as e:
            print(f"ç›‘æ§å‡ºé”™: {e}")

    # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
    schedule.every().hour.do(check_updates)

    while True:
        schedule.run_pending()
        time.sleep(60)
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–
- ä½¿ç”¨åˆ†é¡µæŸ¥è¯¢å¤§é‡æ•°æ®
- ç¼“å­˜é¢‘ç¹è®¿é—®çš„å›¾è°±æ•°æ®
- æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚

### 2. é”™è¯¯å¤„ç†
```python
def safe_api_call(func, *args, **kwargs):
    """å®‰å…¨çš„APIè°ƒç”¨åŒ…è£…å™¨"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                raise e
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

### 3. æ•°æ®éªŒè¯
```python
def validate_graph_data(graph_data):
    """éªŒè¯å›¾è°±æ•°æ®å®Œæ•´æ€§"""
    required_fields = ['nodes', 'edges']

    if not all(field in graph_data['data']['graph'] for field in required_fields):
        raise ValueError("å›¾è°±æ•°æ®æ ¼å¼ä¸å®Œæ•´")

    # éªŒè¯èŠ‚ç‚¹å’Œè¾¹çš„å¼•ç”¨ä¸€è‡´æ€§
    node_ids = {node['id'] for node in graph_data['data']['graph']['nodes']}

    for edge in graph_data['data']['graph']['edges']:
        if edge['source'] not in node_ids or edge['target'] not in node_ids:
            print(f"è­¦å‘Š: è¾¹å¼•ç”¨äº†ä¸å­˜åœ¨çš„èŠ‚ç‚¹ {edge['source']} -> {edge['target']}")
```

## ğŸ“ˆ æ•°æ®å¯è§†åŒ–

### ä½¿ç”¨ NetworkX å’Œ Matplotlib
```python
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

def visualize_knowledge_graph(graph_data, output_file="knowledge_graph.png"):
    """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""

    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False

    # åˆ›å»ºå›¾
    G = nx.Graph()

    # æ·»åŠ èŠ‚ç‚¹
    for node in graph_data['data']['graph']['nodes']:
        G.add_node(
            node['id'],
            entity_type=node['entity_type'],
            pagerank=node.get('pagerank', 0)
        )

    # æ·»åŠ è¾¹
    for edge in graph_data['data']['graph']['edges']:
        G.add_edge(
            edge['source'],
            edge['target'],
            weight=edge.get('weight', 1)
        )

    # å¸ƒå±€
    pos = nx.spring_layout(G, k=1, iterations=50)

    # ç»˜åˆ¶
    plt.figure(figsize=(15, 10))

    # æŒ‰å®ä½“ç±»å‹ç€è‰²
    color_map = {
        'PERSON': 'lightblue',
        'ORGANIZATION': 'lightgreen',
        'CATEGORY': 'lightyellow',
        'GEO': 'lightcoral',
        'EVENT': 'lightpink'
    }

    node_colors = [color_map.get(G.nodes[node].get('entity_type', ''), 'gray')
                   for node in G.nodes()]

    # ç»˜åˆ¶ç½‘ç»œ
    nx.draw(G, pos,
            node_color=node_colors,
            node_size=300,
            font_size=8,
            font_weight='bold',
            with_labels=True,
            edge_color='gray',
            alpha=0.7)

    plt.title("RAGFlow çŸ¥è¯†å›¾è°±å¯è§†åŒ–", fontsize=16)
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.show()
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### å¸¸è§é—®é¢˜

**Q: API è¿”å› 404 é”™è¯¯ï¼Ÿ**
A: æ£€æŸ¥æ•°æ®é›†IDæ˜¯å¦æ­£ç¡®ï¼Œç¡®è®¤çŸ¥è¯†å›¾è°±åŠŸèƒ½å·²å¯ç”¨

**Q: å›¾è°±æ•°æ®ä¸ºç©ºï¼Ÿ**
A: ç¡®è®¤æ–‡æ¡£å·²å®Œæˆè§£æï¼ŒçŸ¥è¯†å›¾è°±æ„å»ºéœ€è¦æ—¶é—´

**Q: ä¸­æ–‡æ˜¾ç¤ºä¹±ç ï¼Ÿ**
A: ç¡®ä¿è¯·æ±‚å¤´åŒ…å«æ­£ç¡®çš„ç¼–ç ï¼š`Content-Type: application/json; charset=utf-8`

### æ•…éšœæ’æŸ¥æ­¥éª¤
1. æ£€æŸ¥ RAGFlow æœåŠ¡çŠ¶æ€ï¼š`docker-compose ps`
2. éªŒè¯ API å¯†é’¥æœ‰æ•ˆæ€§
3. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸
4. æ£€æŸ¥è¯·æ±‚æ ¼å¼å’Œå‚æ•°
5. æŸ¥çœ‹æœåŠ¡æ—¥å¿—ï¼š`docker logs ragflow-server`

### è”ç³»æ–¹å¼
- é¡¹ç›®åœ°å€: https://github.com/infiniflow/ragflow
- æ–‡æ¡£åœ°å€: https://ragflow.io/docs
- ç¤¾åŒºæ”¯æŒ: https://github.com/infiniflow/ragflow/discussions
